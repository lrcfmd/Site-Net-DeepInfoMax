label: "compact_dim_Adam_supervised_veryminor"

###############
#Features used#
###############

interaction_feature_size: 1
Interaction_Features:
- kwargs:
    polynomial_degree: 1
  name: distance_matrix


site_feature_size: 8
embedding_size: 0
Site_Features:
- Featurizer_KArgs:
    properties:
    - Number
    - AtomicWeight
    - Row
    - Column
    - FirstIonizationEnergy
    - Electronegativity
    - AtomicRadius
    - Density
  Featurizer_PArgs: []
  name: SiteElementalProperty

###########################
#Training Hyperparameters#
###########################

#Assign relative weights to the global deep infomax loss
DIM_loss_global: 0.01
Prior_loss_global: 0.0
KL_loss_global: 0.0

#Assign relative weights to the local deep infomax loss
DIM_loss_local: 0.01
Prior_loss_local: 0.0
KL_loss_local: 0.0

#Dynamic beatch size is based on number of unique sites not number of crystals
dynamic_batch: !!bool True
Batch_Size: 1200

#Pytorch lightning optimizer name and hyper parameters
Learning_Rate: 0.0001
Optimizer:
  Kwargs: {}
  Name: Adam

#Allows arguments to be dynamically fed to the pytorch lightning trainer
Trainer kwargs:
  accumulate_grad_batches: 1
  auto_lr_find: false
  max_epochs: 100000000

#Limit on dataset size, arbitrarily high number to disable
Max_Samples: 1000000000

#Multi-task-specific
grad_accumulate: 20
labels: 1000

#######################
#Model Hyperparameters#
#######################

activation_function: relu6
set_norm: layer
lin_norm: layer

site_dim_per_head: 64
site_bottleneck: 16
site_dot_hidden_layers:
- 128
site_dot_space: 256
attention_dim_interaction: 64

attention_heads: 1
attention_blocks: 1
distance_cutoff: -1 #No cut off, positive values add a cut-off measured in angstroms, made performance worse in testing
attention_hidden_layers:
- 128
k_softmax: -1 #No k softmax, available functionality but unused

#Layers post attention blocks
pre_pool_layers:
- 256
- 512
sym_func: mean
post_pool_layers:
- 256
- 256
global_dot_hidden_layers:
- 512
global_dot_space: 1024
last_af_func: TReLU